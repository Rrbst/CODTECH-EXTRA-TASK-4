import numpy as np
import keras
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences

# Sample data: Here, you would use a large dataset related to a specific topic.
corpus = [
    "Artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems.",
    "Machine learning is a subset of AI that involves the creation of algorithms that allow computers to learn from data.",
    "Deep learning is a branch of machine learning based on artificial neural networks that is used to model complex patterns in data.",
    "Natural language processing allows machines to understand and respond to human language.",
    "Generative models can create new data based on learned patterns."
]

# Preprocessing the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(corpus)
total_words = len(tokenizer.word_index) + 1

# Create input sequences and corresponding labels for training
input_sequences = []
for line in corpus:
    token_list = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(token_list)):
        n_gram_sequence = token_list[:i + 1]
        input_sequences.append(n_gram_sequence)

max_sequence_length = max([len(x) for x in input_sequences])
input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')

# Define features (X) and labels (y)
X, y = input_sequences[:, :-1], input_sequences[:, -1]
y = keras.utils.to_categorical(y, num_classes=total_words)

# Building the LSTM model
model = Sequential()
model.add(LSTM(100, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))
model.add(LSTM(100))
model.add(Dropout(0.2))
model.add(Dense(total_words, activation='softmax'))

# Compile and train the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, y, epochs=100, verbose=1)

# Function to generate text
def generate_text(seed_text, next_words=50):
    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=max_sequence_length - 1, padding='pre')
        predicted = model.predict(token_list, verbose=0)
        predicted_word = tokenizer.index_word[np.argmax(predicted)]
        seed_text += " " + predicted_word
    return seed_text

# Generate text based on a seed sentence
seed_text = "Artificial intelligence"
generated_text = generate_text(seed_text, next_words=50)
print("Generated Text:", generated_text)

Tokenizer: Tokenizes the corpus into a sequence of integers.
LSTM Model: An LSTM network is built to learn the text patterns.
Text Generation: A seed sentence is given, and the model generates new words based on that seed.
